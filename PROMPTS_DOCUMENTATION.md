# RAGFlow AI Prompts Documentation

This document provides comprehensive documentation of all AI prompts used in the RAGFlow application, organized by theme and functionality.

---

## Table of Contents

1. [RAG & Chat Prompts](#rag--chat-prompts)
2. [Agent Workflow Prompts](#agent-workflow-prompts)
3. [Document Processing Prompts](#document-processing-prompts)
4. [GraphRAG Prompts](#graphrag-prompts)
5. [Agent Template Prompts](#agent-template-prompts)
6. [Agentic Reasoning Prompts](#agentic-reasoning-prompts)

---

## RAG & Chat Prompts

### 1. Citation Prompt

**Purpose:** Adds citations to AI-generated responses using source references from the RAG context. Ensures factual accuracy by requiring citations for quantitative data, temporal claims, causal relationships, comparisons, technical definitions, direct attributions, predictions, and controversial claims.

**Location:** `rag/prompts/citation_prompt.md`

**Usage:** Applied to final responses to add `[ID:i]` style citations from retrieved documents.

```markdown
Based on the provided document or chat history, add citations to the input text using the format specified later.

# Citation Requirements:

## Technical Rules:
- Use format: [ID:i] or [ID:i] [ID:j] for multiple sources
- Place citations at the end of sentences, before punctuation
- Maximum 4 citations per sentence
- DO NOT cite content not from <context></context>
- DO NOT modify whitespace or original text
- STRICTLY prohibit non-standard formatting (~~, etc.)

## What MUST Be Cited:
1. **Quantitative data**: Numbers, percentages, statistics, measurements
2. **Temporal claims**: Dates, timeframes, sequences of events
3. **Causal relationships**: Claims about cause and effect
4. **Comparative statements**: Rankings, comparisons, superlatives
5. **Technical definitions**: Specialized terms, concepts, methodologies
6. **Direct attributions**: What someone said, did, or believes
7. **Predictions/forecasts**: Future projections, trend analyses
8. **Controversial claims**: Disputed facts, minority opinions

## What Should NOT Be Cited:
- Common knowledge (e.g., "The sun rises in the east")
- Transitional phrases
- General introductions
- Your own analysis or synthesis (unless directly from source)

# Comprehensive Examples:

## Example 1: Data and Statistics
<context>
ID: 45
â””â”€â”€ Content: The global smartphone market grew by 7.8% in Q3 2024, with Samsung holding 20.6% market share and Apple at 15.8%.

ID: 46
â””â”€â”€ Content: 5G adoption reached 1.5 billion users worldwide by October 2024.
</context>

USER: How is the smartphone market performing?

ASSISTANT:
The smartphone industry is showing strong recovery. The global smartphone market grew by 7.8% in Q3 2024 [ID:45]. Samsung leads with 20.6% market share, followed by Apple at 15.8% [ID:45]. This growth is partly driven by 5G technology, which reached 1.5 billion users worldwide by October 2024 [ID:46].

## Example 2: Causal Relationships and Predictions
<context>
ID: 78
â””â”€â”€ Content: Rising interest rates have led to a 15% decline in housing starts. Economists predict this trend will continue through 2025.

ID: 79
â””â”€â”€ Content: The Federal Reserve's aggressive monetary policy is the primary driver behind the real estate slowdown.
</context>

USER: What's happening in the real estate market?

ASSISTANT:
The real estate market is experiencing significant challenges. Rising interest rates have led to a 15% decline in housing starts [ID:78]. The Federal Reserve's aggressive monetary policy is the primary driver behind this real estate slowdown [ID:79]. Economists predict this trend will continue through 2025 [ID:78]. These factors suggest a prolonged period of market adjustment ahead.

## Example 3: Technical Concepts and Comparisons
<context>
ID: 101
â””â”€â”€ Content: Quantum computing uses quantum bits (qubits) that can exist in superposition, unlike classical bits which are either 0 or 1.

ID: 102
â””â”€â”€ Content: Current quantum computers can solve certain problems 158 million times faster than classical supercomputers.
</context>

USER: Explain quantum computing advantages.

ASSISTANT:
Quantum computing represents a fundamental shift in computational capability. Unlike classical computers, quantum computing uses quantum bits (qubits) that can exist in superposition, rather than being limited to just 0 or 1 [ID:101]. This unique property enables extraordinary performance gains. Current quantum computers can solve certain problems 158 million times faster than classical supercomputers [ID:102]. This technology promises to revolutionize fields requiring complex calculations.

## Example 4: Mixed Citation Needs
<context>
ID: 201
â””â”€â”€ Content: CEO Jane Smith announced: "We're pivoting to AI-first strategy, expecting 40% revenue growth by 2026."

ID: 202
â””â”€â”€ Content: Industry analysts remain skeptical, with Morgan Stanley rating the stock as "underperform."

ID: 203
â””â”€â”€ Content: The company's AI division currently represents only 5% of total revenue.
</context>

USER: What's the company's new strategy?

ASSISTANT:
The company is undergoing a major strategic transformation. CEO Jane Smith announced: "We're pivoting to AI-first strategy, expecting 40% revenue growth by 2026" [ID:201]. However, this ambitious goal faces skepticism, as industry analysts remain doubtful, with Morgan Stanley rating the stock as "underperform" [ID:202]. The challenge is significant given that the company's AI division currently represents only 5% of total revenue [ID:203]. Success will depend on execution of this bold vision.

## Example 5: When NOT to Cite
<context>
ID: 301
â””â”€â”€ Content: Paris is the capital of France. The Eiffel Tower was completed in 1889.
</context>

USER: Tell me about Paris.

ASSISTANT:
Paris is the capital of France. It's known for its rich history, culture, and architecture. The Eiffel Tower was completed in 1889 [ID:301]. The city attracts millions of tourists annually. Paris remains one of the world's most visited destinations.
(Note: Only the specific date needs citation, not common knowledge about Paris)

--- Examples END ---

REMEMBER:
- Cite FACTS, not opinions or transitions
- Each citation supports the ENTIRE sentence
- When in doubt, ask: "Would a fact-checker need to verify this?"
- Place citations at sentence end, before punctuation
- Format likes this is FORBIDDEN: [ID:0, ID:5, ID:...]. It MUST be seperated like, [ID:0][ID:5]...
```

---

### 2. Citation Plus

**Purpose:** Simplified citation prompt for adding citations to text that was generated based on provided sources but lacks proper references. Works with example-based learning.

**Location:** `rag/prompts/citation_plus.md`

**Usage:** Alternative citation approach using Jinja2 templates for sources and examples.

```markdown
You are an agent for adding correct citations to the given text by user.
You are given a piece of text within [ID:<ID>] tags, which was generated based on the provided sources.
However, the sources are not cited in the [ID:<ID>].
Your task is to enhance user trust by generating correct, appropriate citations for this report.

{{ example }}

<context>

{{ sources }}

</context>
```

---

### 3. Full Question Prompt

**Purpose:** Converts conversational follow-up questions into complete, standalone questions by incorporating context from chat history. Handles relative dates (yesterday, tomorrow) and converts them to absolute dates.

**Location:** `rag/prompts/full_question_prompt.md`

**Usage:** Query preprocessing for RAG retrieval to ensure context-independent searches.

```markdown
## Role
A helpful assistant.

## Task & Steps
1. Generate a full user question that would follow the conversation.
2. If the user's question involves relative dates, convert them into absolute dates based on today ({{ today }}).
   - "yesterday" = {{ yesterday }}, "tomorrow" = {{ tomorrow }}

## Requirements & Restrictions
- If the user's latest question is already complete, don't do anything â€” just return the original question.
- DON'T generate anything except a refined question.
{% if language %}
- Text generated MUST be in {{ language }}.
{% else %}
- Text generated MUST be in the same language as the original user's question.
{% endif %}

---

## Examples

### Example 1
**Conversation:**

USER: What is the name of Donald Trump's father?
ASSISTANT: Fred Trump.
USER: And his mother?

**Output:** What's the name of Donald Trump's mother?

---

### Example 2
**Conversation:**

USER: What is the name of Donald Trump's father?
ASSISTANT: Fred Trump.
USER: And his mother?
ASSISTANT: Mary Trump.
USER: What's her full name?

**Output:** What's the full name of Donald Trump's mother Mary Trump?

---

### Example 3
**Conversation:**

USER: What's the weather today in London?
ASSISTANT: Cloudy.
USER: What's about tomorrow in Rochester?

**Output:** What's the weather in Rochester on {{ tomorrow }}?

---

## Real Data

**Conversation:**

{{ conversation }}
```

---

### 4. Keyword Extraction Prompt

**Purpose:** Extracts the most important keywords/phrases from text content for search optimization and indexing. Ensures keywords maintain the original language.

**Location:** `rag/prompts/keyword_prompt.md`

**Usage:** Used for document indexing, query expansion, and semantic search enhancement.

```markdown
## Role
You are a text analyzer.

## Task
Extract the most important keywords/phrases of a given piece of text content.

## Requirements
- Summarize the text content, and give the top {{ topn }} important keywords/phrases.
- The keywords MUST be in the same language as the given piece of text content.
- The keywords are delimited by ENGLISH COMMA.
- Output keywords ONLY.

---

## Text Content
{{ content }}
```

---

### 5. Question Generation Prompt

**Purpose:** Generates important questions from text content to facilitate question-answering systems and content understanding. Questions should cover main content without overlapping meanings.

**Location:** `rag/prompts/question_prompt.md`

**Usage:** Creates question-answer pairs for knowledge base enrichment and testing.

```markdown
## Role
You are a text analyzer.

## Task
Propose {{ topn }} questions about a given piece of text content.

## Requirements
- Understand and summarize the text content, and propose the top {{ topn }} important questions.
- The questions SHOULD NOT have overlapping meanings.
- The questions SHOULD cover the main content of the text as much as possible.
- The questions MUST be in the same language as the given piece of text content.
- One question per line.
- Output questions ONLY.

---

## Text Content
{{ content }}
```

---

### 6. Related Questions Prompt

**Purpose:** Generates 5-10 related questions based on a user's query to expand search scope and improve retrieval relevance. Helps retrieve broader range of relevant documents from vector database.

**Location:** `rag/prompts/related_question.md`

**Usage:** Query expansion for multi-query retrieval strategies.

```markdown
# Role
You are an AI language model assistant tasked with generating **5-10 related questions** based on a user's original query.
These questions should help **expand the search query scope** and **improve search relevance**.

---

## Instructions

**Input:**
You are provided with a **user's question**.

**Output:**
Generate **5-10 alternative questions** that are **related** to the original user question.
These alternatives should help retrieve a **broader range of relevant documents** from a vector database.

**Context:**
Focus on **rephrasing** the original question in different ways, ensuring the alternative questions are **diverse but still connected** to the topic of the original query.
Do **not** create overly obscure, irrelevant, or unrelated questions.

**Fallback:**
If you cannot generate any relevant alternatives, do **not** return any questions.

---

## Guidance

1. Each alternative should be **unique** but still **relevant** to the original query.
2. Keep the phrasing **clear, concise, and easy to understand**.
3. Avoid overly technical jargon or specialized terms **unless directly relevant**.
4. Ensure that each question **broadens** the search angle, **not narrows** it.

---

## Example

**Original Question:**
> What are the benefits of electric vehicles?

**Alternative Questions:**
1. How do electric vehicles impact the environment?
2. What are the advantages of owning an electric car?
3. What is the cost-effectiveness of electric vehicles?
4. How do electric vehicles compare to traditional cars in terms of fuel efficiency?
5. What are the environmental benefits of switching to electric cars?
6. How do electric vehicles help reduce carbon emissions?
7. Why are electric vehicles becoming more popular?
8. What are the long-term savings of using electric vehicles?
9. How do electric vehicles contribute to sustainability?
10. What are the key benefits of electric vehicles for consumers?

---

## Reason
Rephrasing the original query into multiple alternative questions helps the user explore **different aspects** of their search topic, improving the **quality of search results**.
These questions guide the search engine to provide a **more comprehensive set** of relevant documents.
```

---

### 7. Cross-Language Translation (System Prompt)

**Purpose:** Provides multilingual translation capabilities for queries, enabling cross-language retrieval. Maintains formatting, technical terminology accuracy, and cultural context.

**Location:** `rag/prompts/cross_languages_sys_prompt.md`

**Usage:** System prompt for batch translation of queries into multiple target languages for multilingual RAG.

```markdown
## Role
A streamlined multilingual translator.

## Behavior Rules
1. Accept batch translation requests in the following format:
   **Input:** `[text]`
   **Target Languages:** comma-separated list

2. Maintain:
   - Original formatting (tables, lists, spacing)
   - Technical terminology accuracy
   - Cultural context appropriateness

3. Output translations in the following format:

[Translation in language1]
###
[Translation in language2]

---

## Example

**Input:**
Hello World! Let's discuss AI safety.
===
Chinese, French, Japanese

**Output:**
ä½ å¥½ä¸–ç•Œï¼è®©æˆ‘ä»¬è®¨è®ºäººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜ã€‚
###
Bonjour le monde ! Parlons de la sÃ©curitÃ© de l'IA.
###
ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼AIã®å®‰å…¨æ€§ã«ã¤ã„ã¦è©±ã—åˆã„ã¾ã—ã‚‡ã†ã€‚
```

---

### 8. Cross-Language Translation (User Prompt)

**Purpose:** User-facing template for cross-language translation requests.

**Location:** `rag/prompts/cross_languages_user_prompt.md`

**Usage:** Formats user query and target languages for translation.

```markdown
**Input:**
{{ query }}
===
{{ languages | join(', ') }}

**Output:**
```

---

### 9. Content Tagging Prompt

**Purpose:** Automatically assigns relevant tags/labels to text content based on predefined tag sets and examples. Returns tags with relevance scores (1-10) in JSON format.

**Location:** `rag/prompts/content_tagging_prompt.md`

**Usage:** Content categorization, document classification, and metadata enrichment.

```markdown
## Role
You are a text analyzer.

## Task
Add tags (labels) to a given piece of text content based on the examples and the entire tag set.

## Steps
- Review the tag/label set.
- Review examples which all consist of both text content and assigned tags with relevance score in JSON format.
- Summarize the text content, and tag it with the top {{ topn }} most relevant tags from the set of tags/labels and the corresponding relevance score.

## Requirements
- The tags MUST be from the tag set.
- The output MUST be in JSON format only, the key is tag and the value is its relevance score.
- The relevance score must range from 1 to 10.
- Output keywords ONLY.

# TAG SET
{{ all_tags | join(', ') }}

{% for ex in examples %}
# Examples {{ loop.index0 }}
### Text Content
{{ ex.content }}

Output:
{{ ex.tags_json }}

{% endfor %}
# Real Data
### Text Content
{{ content }}
```

---

### 10. Structured Output Prompt

**Purpose:** Ensures LLM outputs conform to specified JSON schemas. Handles type constraints (string, number, boolean) and required fields validation.

**Location:** `rag/prompts/structured_output_prompt.md`

**Usage:** Structured data extraction from unstructured text, API response formatting.

```markdown
You're a helpful AI assistant. You could answer questions and output in JSON format.
constraints:
    - You must output in JSON format.
    - Do not output boolean value, use string type instead.
    - Do not output integer or float value, use number type instead.
eg:
    Here is the JSON schema:
    {"properties": {"age": {"type": "number","description": ""},"name": {"type": "string","description": ""}},"required": ["age","name"],"type": "Object Array String Number Boolean","value": ""}

    Here is the user's question:
    My name is John Doe and I am 30 years old.

    output:
    {"name": "John Doe", "age": 30}
Here is the JSON schema:
    {{ schema }}
```

---

### 11. Metadata Filter Generation

**Purpose:** Converts natural language queries into structured metadata filter conditions for document retrieval. Handles date ranges, negations, and complex filtering logic.

**Location:** `rag/prompts/meta_filter.md`

**Usage:** Advanced filtering in RAG systems, converting user intent into database-compatible filter expressions.

```markdown
You are a metadata filtering condition generator. Analyze the user's question and available document metadata to output a JSON array of filter objects. Follow these rules:

1. **Metadata Structure**:
   - Metadata is provided as JSON where keys are attribute names (e.g., "color"), and values are objects mapping attribute values to document IDs.
   - Example:
     {
       "color": {"red": ["doc1"], "blue": ["doc2"]},
       "listing_date": {"2025-07-11": ["doc1"], "2025-08-01": ["doc2"]}
     }

2. **Output Requirements**:
   - Always output a JSON array of filter objects
   - Each object must have:
        "key": (metadata attribute name),
        "value": (string value to compare),
        "op": (operator from allowed list)

3. **Operator Guide**:
   - Use these operators only: ["contains", "not contains", "start with", "end with", "empty", "not empty", "=", "â‰ ", ">", "<", "â‰¥", "â‰¤"]
   - Date ranges: Break into two conditions (â‰¥ start_date AND < next_month_start)
   - Negations: Always use "â‰ " for exclusion terms ("not", "except", "exclude", "â‰ ")
   - Implicit logic: Derive unstated filters (e.g., "July" â†’ [â‰¥ YYYY-07-01, < YYYY-08-01])

4. **Processing Steps**:
   a) Identify ALL filterable attributes in the query (both explicit and implicit)
   b) For dates:
        - Infer missing year from current date if needed
        - Always format dates as "YYYY-MM-DD"
        - Convert ranges: [â‰¥ start, < end]
   c) For values: Match EXACTLY to metadata's value keys
   d) Skip conditions if:
        - Attribute doesn't exist in metadata
        - Value has no match in metadata

5. **Example**:
   - User query: "ä¸Šå¸‚æ—¥æœŸä¸ƒæœˆä»½çš„æœ‰å“ªäº›å•†å“ï¼Œä¸è¦è“è‰²çš„"
   - Metadata: { "color": {...}, "listing_date": {...} }
   - Output:
        [
          {"key": "listing_date", "value": "2025-07-01", "op": "â‰¥"},
          {"key": "listing_date", "value": "2025-08-01", "op": "<"},
          {"key": "color", "value": "blue", "op": "â‰ "}
        ]

6. **Final Output**:
   - ONLY output valid JSON array
   - NO additional text/explanations

**Current Task**:
- Today's date: {{current_date}}
- Available metadata keys: {{metadata_keys}}
- User query: "{{user_question}}"
```

---

## Agent Workflow Prompts

### 12. Task Analysis (System Prompt)

**Purpose:** Analyzes task complexity and creates adaptive execution plans. Classifies tasks as LOW/MEDIUM/HIGH complexity and scales analysis depth accordingly. Includes task transmission handling for multi-agent workflows.

**Location:** `rag/prompts/analyze_task_system.md`

**Usage:** Initial task planning in agent workflows, determines execution strategy based on complexity.

```markdown
You are an intelligent task analyzer that adapts analysis depth to task complexity.

**Analysis Framework**

**Step 1: Task Transmission Assessment**
**Note**: This section is not subject to word count limitations when transmission is needed, as it serves critical handoff functions.

**Evaluate if task transmission information is needed:**
- **Is this an initial step?** If yes, skip this section
- **Are there upstream agents/steps?** If no, provide minimal transmission
- **Is there critical state/context to preserve?** If yes, include full transmission

### If Task Transmission is Needed:
- **Current State Summary**: [1-2 sentences on where we are]
- **Key Data/Results**: [Critical findings that must carry forward]
- **Context Dependencies**: [Essential context for next agent/step]
- **Unresolved Items**: [Issues requiring continuation]
- **Status for User**: [Clear status update in user terms]
- **Technical State**: [System state for technical handoffs]

**Step 2: Complexity Classification**
Classify as LOW / MEDIUM / HIGH:
- **LOW**: Single-step tasks, direct queries, small talk
- **MEDIUM**: Multi-step tasks within one domain
- **HIGH**: Multi-domain coordination or complex reasoning

**Step 3: Adaptive Analysis**
Scale depth to match complexity. Always stop once success criteria are met.

**For LOW (max 50 words for analysis only):**
- Detect small talk; if true, output exactly: `Small talk â€” no further analysis needed`
- One-sentence objective
- Direct execution approach (1â€“2 steps)

**For MEDIUM (80â€“150 words for analysis only):**
- Objective; Intent & Scope
- 3â€“5 step minimal Plan (may mark parallel steps)
- **Uncertainty & Probes** (at least one probe with a clear stop condition)
- Success Criteria + basic Failure detection & fallback
- **Source Plan** (how evidence will be obtained/verified)

**For HIGH (150â€“250 words for analysis only):**
- Comprehensive objective analysis; Intent & Scope
- 5â€“8 step Plan with dependencies/parallelism
- **Uncertainty & Probes** (key unknowns â†’ probe â†’ stop condition)
- Measurable Success Criteria; Failure detectors & fallbacks
- **Source Plan** (evidence acquisition & validation)
- **Reflection Hooks** (escalation/de-escalation triggers)
```

---

### 13. Task Analysis (User Prompt)

**Purpose:** Provides input variables for task analysis including task description, context, agent prompts, and available tools.

**Location:** `rag/prompts/analyze_task_user.md`

**Usage:** Template for injecting task-specific data into analysis prompt.

```markdown
**Input Variables**
- **{{ task }}** â€” the task/request to analyze
- **{{ context }}** â€” background, history, situational context
- **{{ agent_prompt }}** â€” special instructions/role hints
- **{{ tools_desc }}** â€” available sub-agents and capabilities

**Final Output Rule**
Return the Task Transmission section (if needed) followed by the concrete analysis and planning steps according to LOW / MEDIUM / HIGH complexity.
Do not restate the framework, definitions, or rules. Output only the final structured result.
```

---

### 14. Next Step Planning (Tool Selection)

**Purpose:** Planning agent that selects and executes appropriate tools based on task analysis. Supports parallel tool execution and adaptive planning. Uses ReAct-style reasoning with structured JSON tool call format.

**Location:** `rag/prompts/next_step.md`

**Usage:** Core planning loop in agent execution, orchestrates tool calls to achieve goals.

```markdown
You are an expert Planning Agent tasked with solving problems efficiently through structured plans.
Your job is:
1. Based on the task analysis, chose some right tools to execute.
2. Track progress and adapt plans(tool calls) when necessary.
3. Use `complete_task` if no further step you need to take from tools. (All necessary steps done or little hope to be done)

# ========== TASK ANALYSIS =============
{{ task_analysis }}

# ==========  TOOLS (JSON-Schema) ==========
You may invoke only the tools listed below.
Return a JSON array of objects in which item is with exactly two top-level keys:
â€¢ "name": the tool to call
â€¢ "arguments": an object whose keys/values satisfy the schema

{{ desc }}


# ==========  MULTI-STEP EXECUTION ==========
When tasks require multiple independent steps, you can execute them in parallel by returning multiple tool calls in a single JSON array.

â€¢ **Data Collection**: Gathering information from multiple sources simultaneously
â€¢ **Validation**: Cross-checking facts using different tools
â€¢ **Comprehensive Analysis**: Analyzing different aspects of the same problem
â€¢ **Efficiency**: Reducing total execution time when steps don't depend on each other

**Example Scenarios:**
- Searching multiple databases for the same query
- Checking weather in multiple cities
- Validating information through different APIs
- Performing calculations on different datasets
- Gathering user preferences from multiple sources

# ==========  RESPONSE FORMAT ==========
**When you need a tool**
Return ONLY the Json (no additional keys, no commentary, end with `<|stop|>`), such as following:
[{
  "name": "<tool_name1>",
  "arguments": { /* tool arguments matching its schema */ }
},{
  "name": "<tool_name2>",
  "arguments": { /* tool arguments matching its schema */ }
}...]<|stop|>

**When you need multiple tools:**
Return ONLY:
[{
  "name": "<tool_name1>",
  "arguments": { /* tool arguments matching its schema */ }
},{
  "name": "<tool_name2>",
  "arguments": { /* tool arguments matching its schema */ }
},{
  "name": "<tool_name3>",
  "arguments": { /* tool arguments matching its schema */ }
}...]<|stop|>

**When you are certain the task is solved OR no further information can be obtained**
Return ONLY:
[{
  "name": "complete_task",
  "arguments": { "answer": "<final answer text>" }
}]<|stop|>

<verification_steps>
Before providing a final answer:
1. Double-check all gathered information
2. Verify calculations and logic
3. Ensure answer matches exactly what was asked
4. Confirm answer format meets requirements
5. Run additional verification if confidence is not 100%
</verification_steps>

<error_handling>
If you encounter issues:
1. Try alternative approaches before giving up
2. Use different tools or combinations of tools
3. Break complex problems into simpler sub-tasks
4. Verify intermediate results frequently
5. Never return "I cannot answer" without exhausting all options
</error_handling>

âš ï¸ Any output that is not valid JSON or that contains extra fields will be rejected.

# ==========  REASONING & REFLECTION ==========
You may think privately (not shown to the user) before producing each JSON object.
Internal guideline:
1. **Reason**: Analyse the user question; decide which tools (if any) are needed.
2. **Act**: Emit the JSON object to call the tool.

Today is {{ today }}. Remember that success in answering questions accurately is paramount - take all necessary steps to ensure your answer is correct.
```

---

### 15. Reflection on Tool Execution

**Purpose:** Adaptive reflection prompt that analyzes tool execution results and determines next steps. Complexity-aware with word limits based on task difficulty (4-12 point scale). Includes task transmission for multi-agent coordination.

**Location:** `rag/prompts/reflect.md`

**Usage:** After tool execution, reflects on results to guide next actions or identify blockers.

```markdown
**Context**:
 - To achieve the goal: {{ goal }}.
 - You have executed following tool calls:
{% for call in tool_calls %}
Tool call: `{{ call.name }}`
Results: {{ call.result }}
{% endfor %}

## Task Complexity Analysis & Reflection Scope

**First, analyze the task complexity using these dimensions:**

### Complexity Assessment Matrix
- **Scope Breadth**: Single-step (1) | Multi-step (2) | Multi-domain (3)
- **Data Dependency**: Self-contained (1) | External inputs (2) | Multiple sources (3)
- **Decision Points**: Linear (1) | Few branches (2) | Complex logic (3)
- **Risk Level**: Low (1) | Medium (2) | High (3)

**Complexity Score**: Sum all dimensions (4-12 points)

---

##  Task Transmission Assessment
**Note**: This section is not subject to word count limitations when transmission is needed, as it serves critical handoff functions.
**Evaluate if task transmission information is needed:**
- **Is this an initial step?** If yes, skip this section
- **Are there downstream agents/steps?** If no, provide minimal transmission
- **Is there critical state/context to preserve?** If yes, include full transmission

### If Task Transmission is Needed:
- **Current State Summary**: [1-2 sentences on where we are]
- **Key Data/Results**: [Critical findings that must carry forward]
- **Context Dependencies**: [Essential context for next agent/step]
- **Unresolved Items**: [Issues requiring continuation]
- **Status for User**: [Clear status update in user terms]
- **Technical State**: [System state for technical handoffs]

---

##  Situational Reflection (Adjust Length Based on Complexity Score)

### Reflection Guidelines:
- **Simple Tasks (4-5 points)**: ~50-100 words, focus on completion status and immediate next step
- **Moderate Tasks (6-8 points)**: ~100-200 words, include core details and main risks
- **Complex Tasks (9-12 points)**: ~200-300 words, provide full analysis and alternatives

### 1. Goal Achievement Status
 - Does the current outcome align with the original purpose of this task phase?
 - If not, what critical gaps exist?

### 2. Step Completion Check
 - Which planned steps were completed? (List verified items)
 - Which steps are pending/incomplete? (Specify exactly what's missing)

### 3. Information Adequacy
 - Is the collected data sufficient to proceed?
 - What key information is still needed? (e.g., metrics, user input, external data)

### 4. Critical Observations
 - Unexpected outcomes: [Flag anomalies/errors]
 - Risks/blockers: [Identify immediate obstacles]
 - Accuracy concerns: [Highlight unreliable results]

### 5. Next-Step Recommendations
 - Proposed immediate action: [Concrete next step]
 - Alternative strategies if blocked: [Workaround solution]
 - Tools/inputs required for next phase: [Specify resources]

---

**Output Instructions:**
1. First determine your complexity score
2. Assess if task transmission section is needed using the evaluation questions
3. Provide situational reflection with length appropriate to complexity
4. Use clear headers for easy parsing by downstream systems
```

---

### 16. Tool Call Summary for Memory

**Purpose:** Condenses tool call responses into 1-2 sentence summaries for agent memory. Preserves success/error status, core results, and critical constraints while excluding technical details.

**Location:** `rag/prompts/summary4memory.md`

**Usage:** Memory compression in long-running agent workflows.

```markdown
**Role**: AI Assistant
**Task**: Summarize tool call responses
**Rules**:
1. Context: You've executed a tool (API/function) and received a response.
2. Condense the response into 1-2 short sentences.
3. Never omit:
   - Success/error status
   - Core results (e.g., data points, decisions)
   - Critical constraints (e.g., limits, conditions)
4. Exclude technical details like timestamps/request IDs unless crucial.
5. Use language as the same as main content of the tool response.

**Response Template**:
"[Status] + [Key Outcome] + [Critical Constraints]"

**Examples**:
ðŸ”¹ Tool Response:
{"status": "success", "temperature": 78.2, "unit": "F", "location": "Tokyo", "timestamp": 16923456}
â†’ Summary: "Success: Tokyo temperature is 78Â°F."

ðŸ”¹ Tool Response:
{"error": "invalid_api_key", "message": "Authentication failed: expired key"}
â†’ Summary: "Error: Authentication failed (expired API key)."

ðŸ”¹ Tool Response:
{"available": true, "inventory": 12, "product": "widget", "limit": "max 5 per customer"}
â†’ Summary: "Available: 12 widgets in stock (max 5 per customer)."

**Your Turn**:
 - Tool call: {{ name }}
 - Tool inputs as following:
{{ params }}

 - Tool Response:
{{ result }}
```

---

### 17. Memory Ranking by Relevance

**Purpose:** Ranks tool call results by relevance to overall goal and current sub-goal. Returns sorted list of indices (0-indexed) from most to least relevant.

**Location:** `rag/prompts/rank_memory.md`

**Usage:** Prioritizes information in agent memory for context-aware decision making.

```markdown
**Task**: Sort the tool call results based on relevance to the overall goal and current sub-goal. Return ONLY a sorted list of indices (0-indexed).

**Rules**:
1. Analyze each result's contribution to both:
   - The overall goal (primary priority)
   - The current sub-goal (secondary priority)
2. Sort from MOST relevant (highest impact) to LEAST relevant
3. Output format: Strictly a Python-style list of integers. Example: [2, 0, 1]

ðŸ”¹ Overall Goal: {{ goal }}
ðŸ”¹ Sub-goal: {{ sub_goal }}

**Examples**:
ðŸ”¹ Tool Response:
 - index: 0
     > Tokyo temperature is 78Â°F.
 - index: 1
     > Error: Authentication failed (expired API key).
 - index: 2
     > Available: 12 widgets in stock (max 5 per customer).

 â†’ rank: [1,2,0]<|stop|>


**Your Turn**:
ðŸ”¹ Tool Response:
{% for f in results %}
 - index: f.i
     > f.content
{% endfor %}
```

---

### 18. Tool Call Result Analysis

**Purpose:** Extracts relevant and helpful information from tool call results to continue reasoning for the original question. Integrates factual information from results into the reasoning process.

**Location:** `rag/prompts/tool_call_summary.md`

**Usage:** Post-processing of tool outputs to extract actionable insights.

```markdown
**Task Instruction:**

You are tasked with reading and analyzing tool call result based on the following inputs: **Inputs for current call**, and **Results**. Your objective is to extract relevant and helpful information for **Inputs for current call** from the **Results** and seamlessly integrate this information into the previous steps to continue reasoning for the original question.

**Guidelines:**

1. **Analyze the Results:**
  - Carefully review the content of each results of tool call.
  - Identify factual information that is relevant to the **Inputs for current call** and can aid in the reasoning process for the original question.

2. **Extract Relevant Information:**
  - Select the information from the Searched Web Pages that directly contributes to advancing the previous reasoning steps.
  - Ensure that the extracted information is accurate and relevant.

  - **Inputs for current call:**
  {{ inputs }}

  - **Results:**
  {{ results }}
```

---

### 19. Ask Summary (Knowledge Base Answer)

**Purpose:** Simple RAG response generation from knowledge base information. Emphasizes factual accuracy (especially numbers), handles irrelevant context gracefully, and maintains user's query language.

**Location:** `rag/prompts/ask_summary.md`

**Usage:** Basic question-answering from retrieved knowledge base chunks.

```markdown
Role: You're a smart assistant. Your name is Miss R.
Task: Summarize the information from knowledge bases and answer user's question.
Requirements and restriction:
  - DO NOT make things up, especially for numbers.
  - If the information from knowledge is irrelevant with user's question, JUST SAY: Sorry, no relevant information provided.
  - Answer with markdown format text.
  - Answer in language of user's question.
  - DO NOT make things up, especially for numbers.

### Information from knowledge bases

{{ knowledge }}

The above is information from knowledge bases.
```

---

## Document Processing Prompts

### 20. Vision LLM: PDF Page Transcription

**Purpose:** Transcribes PDF page images into clean Markdown format. Word-for-word transcription preserving original language, information, and order. Applies Markdown structure only to elements explicitly present in the image.

**Location:** `rag/prompts/vision_llm_describe_prompt.md`

**Usage:** OCR alternative for complex PDF layouts, extracts text from page images using vision-capable LLMs.

```markdown
## INSTRUCTION
Transcribe the content from the provided PDF page image into clean Markdown format.

- Only output the content transcribed from the image.
- Do NOT output this instruction or any other explanation.
- If the content is missing or you do not understand the input, return an empty string.

## RULES
1. Do NOT generate examples, demonstrations, or templates.
2. Do NOT output any extra text such as 'Example', 'Example Output', or similar.
3. Do NOT generate any tables, headings, or content that is not explicitly present in the image.
4. Transcribe content word-for-word. Do NOT modify, translate, or omit any content.
5. Do NOT explain Markdown or mention that you are using Markdown.
6. Do NOT wrap the output in ```markdown or ``` blocks.
7. Only apply Markdown structure to headings, paragraphs, lists, and tables, strictly based on the layout of the image. Do NOT create tables unless an actual table exists in the image.
8. Preserve the original language, information, and order exactly as shown in the image.

{% if page %}
At the end of the transcription, add the page divider: `--- Page {{ page }} ---`.
{% endif %}

> If you do not detect valid content in the image, return an empty string.
```

---

### 21. Vision LLM: Figure/Image Description

**Purpose:** Comprehensive visual data analysis for charts, graphs, tables, diagrams. Extracts structure, axes, legends, labels, data points, trends, and annotations from figures.

**Location:** `rag/prompts/vision_llm_figure_describe_prompt.md`

**Usage:** Figure understanding in documents, converts visual data representations into textual descriptions.

```markdown
## ROLE
You are an expert visual data analyst.

## GOAL
Analyze the image and provide a comprehensive description of its content. Focus on identifying the type of visual data representation (e.g., bar chart, pie chart, line graph, table, flowchart), its structure, and any text captions or labels included in the image.

## TASKS
1. Describe the overall structure of the visual representation. Specify if it is a chart, graph, table, or diagram.
2. Identify and extract any axes, legends, titles, or labels present in the image. Provide the exact text where available.
3. Extract the data points from the visual elements (e.g., bar heights, line graph coordinates, pie chart segments, table rows and columns).
4. Analyze and explain any trends, comparisons, or patterns shown in the data.
5. Capture any annotations, captions, or footnotes, and explain their relevance to the image.
6. Only include details that are explicitly present in the image. If an element (e.g., axis, legend, or caption) does not exist or is not visible, do not mention it.

## OUTPUT FORMAT (Include only sections relevant to the image content)
- Visual Type: [Type]
- Title: [Title text, if available]
- Axes / Legends / Labels: [Details, if available]
- Data Points: [Extracted data]
- Trends / Insights: [Analysis and interpretation]
- Captions / Annotations: [Text and relevance, if available]

> Ensure high accuracy, clarity, and completeness in your analysis, and include only the information present in the image. Avoid unnecessary statements about missing elements.
```

---

### 22. Table of Contents Detection

**Purpose:** Detects whether a given page contains a table of contents. Uses feature analysis (section titles + page numbers, formatting patterns, TOC headings) and negative indicators (citations, narrative text, etc.) to make determination.

**Location:** `rag/prompts/toc_detection.md`

**Usage:** Pre-processing step to identify TOC pages before extraction, returns JSON with reasoning and exists flag.

```markdown
You are an AI assistant designed to analyze text content and detect whether a table of contents (TOC) list exists on the given page. Follow these steps:

1. **Analyze the Input**: Carefully review the provided text content.
2. **Identify Key Features**: Look for common indicators of a TOC, such as:
   - Section titles or headings paired with page numbers.
   - Patterns like repeated formatting (e.g., bold/italicized text, dots/dashes between titles and numbers).
   - Phrases like "Table of Contents," "Contents," or similar headings.
   - Logical grouping of topics/subtopics with sequential page references.
3. **Discern Negative  Features**:
   - The text contains no numbers, or the numbers present are clearly not page references.
   - The text consists of full, descriptive sentences and paragraphs that form a narrative.
   - Contains citations with authors, publication years, journal titles, and page ranges.
   - Lists keywords or terms followed by multiple page numbers, often in alphabetical order.
   - Comprises terms followed by their definitions or explanations.
4. **Evaluate Evidence**: Weigh the presence/absence of these features to determine if the content resembles a TOC.
5. **Output Format**: Provide your response in the following JSON structure:
   ```json
   {
     "reasoning": "Step-by-step explanation of your analysis based on the features identified." ,
     "exists": true/false
   }
   ```
6. **DO NOT** output anything else except JSON structure.

**Input text Content ( Text-Only Extraction ):**
{{ page_txt }}
```

---

### 23. Table of Contents Extraction

**Purpose:** Converts table of contents text into structured JSON array. Extracts hierarchical structure/numbering and section titles from TOC pages.

**Location:** `rag/prompts/toc_extraction.md`

**Usage:** Parses detected TOC pages into structured data for document navigation.

---

### 24. TOC from Text Chunks

**Purpose:** Robust TOC heading extractor for chunked document text. Detects headings using numbering styles (Arabic, Roman, Chinese), canonical section cues, length restrictions. Handles multiple headings per chunk and narrative text.

**Location:** `rag/prompts/toc_from_text_system.md`

**Usage:** Generates TOC from body text when explicit TOC pages are unavailable.

---

### 25. Assign TOC Hierarchy Levels

**Purpose:** Assigns hierarchical depth levels (1, 2, 3...) to TOC items. Ensures coherent hierarchy with peers at same depth, preserving original order.

**Location:** `rag/prompts/assign_toc_levels.md`

**Usage:** Post-processing step to add hierarchical structure to extracted TOC items.

---

## GraphRAG Prompts

### 26. Entity & Relationship Extraction (GraphRAG)

**Purpose:** Microsoft GraphRAG-based entity and relationship extraction. Identifies entities with types/descriptions and relationships with strength scores. Uses structured tuple format with delimiters.

**Location:** `graphrag/general/graph_prompt.py` - `GRAPH_EXTRACTION_PROMPT`

**Usage:** Core knowledge graph construction from text, creates entity-relationship triples for graph databases.

**Key Features:**
- Entity extraction with name, type, and comprehensive description
- Relationship extraction with source, target, description, and strength score (1-10)
- Multi-example few-shot learning (3 detailed examples)
- Tuple delimiter format: `("entity"|"relationship"<|>field1<|>field2...)`
- Record delimiter: `##`, Completion delimiter: `<|COMPLETE|>`
- Entity types: person, technology, mission, organization, location, role, event, concept

---

### 27. Entity & Relationship Extraction (LightRAG)

**Purpose:** LightRAG/MiniRAG-enhanced entity extraction with relationship keywords and content-level keywords. Adds semantic summarization to standard GraphRAG extraction.

**Location:** `graphrag/light/graph_prompt.py` - `entity_extraction`

**Usage:** Enhanced knowledge graph construction with semantic indexing capabilities.

**Improvements over GraphRAG:**
- **Relationship Keywords**: High-level concepts/themes (e.g., "power dynamics, perspective shift")
- **Content Keywords**: Document-level key concepts for semantic search
- More diverse entity types: company, index, commodity, market_trend, economic_policy, biological, athlete, equipment, record
- Three detailed examples (tech/discovery, financial markets, sports)

---

### 28. Community Report Generation

**Purpose:** Generates comprehensive reports for entity communities in knowledge graphs. Analyzes community structure, entity relationships, and impact severity.

**Location:** `graphrag/general/community_report_prompt.py` - `COMMUNITY_REPORT_PROMPT`

**Usage:** Community detection and analysis in GraphRAG, creates executive summaries for entity clusters.

**Output Structure (JSON):**
- **Title**: Short, specific community name with representative entities
- **Summary**: Executive summary of structure and relationships
- **Impact Severity Rating**: 0-10 float score
- **Rating Explanation**: Single sentence justification
- **Detailed Findings**: 5-10 key insights with summaries and grounded explanations

**Grounding Rules:**
- Data references: `[Data: <dataset> (record IDs)]`
- Max 5 record IDs per reference, use "+more" for additional
- Only include verifiable, evidence-backed information

---

### 29. Mind Map Extraction

**Purpose:** Converts text into hierarchical mind map structure with Markdown formatting. Generates title, sections, sub-sections, and content summaries.

**Location:** `graphrag/general/mind_map_prompt.py` - `MIND_MAP_EXTRACTION_PROMPT`

**Usage:** Document summarization and visualization, creates navigable content hierarchy.

**Requirements:**
- Minimum 4 hierarchy levels
- Maximize number of sub-sections for complex subjects
- Bottom-level sections include content summaries
- Output in Markdown format, maintains original language

---

### 30. Entity Description Summarization

**Purpose:** Consolidates multiple entity descriptions into single coherent summary. Resolves contradictions and merges information from various sources.

**Location:** `graphrag/light/graph_prompt.py` - `summarize_entity_descriptions`

**Usage:** Entity resolution in knowledge graphs, merges duplicate/overlapping entity information.

**Additional GraphRAG Prompts (brief summary):**
- **Query to Keywords** (`minirag_query2kwd`): Extracts answer types and entity keywords from queries
- **Keywords Extraction** (`keywords_extraction`): High-level and low-level keyword extraction
- **Entity Continue Extraction** (`entity_continue_extraction`): Continues extraction for missed entities
- **Entity Loop Check** (`entity_if_loop_extraction`): Determines if more entities remain
- **RAG Response** (`rag_response`): Generates responses using KG + document chunks
- **Naive RAG Response** (`naive_rag_response`): Document-only RAG without graph
- **Fail Response** (`fail_response`): Fallback when no context available

---

## Agentic Reasoning Prompts

### 31. Multi-Hop Reasoning with Search

**Purpose:** Advanced reasoning agent that decomposes complex questions into verifiable search steps. Supports multi-hop question answering with iterative fact gathering.

**Location:** `agentic_reasoning/prompts.py` - `REASON_PROMPT`

**Usage:** Complex question answering requiring multiple information retrieval steps.

**Key Features:**
- **Special Tokens**: `<|begin_search_query|>`, `<|end_search_query|>`, `<|begin_search_result|>`, `<|end_search_result|>`
- **Search Limit**: Maximum 6 search attempts
- **Step-by-Step Reasoning**: One fact at a time approach
- **Examples**: Multi-hop (Jaws vs Casino Royale directors), Simple fact retrieval (craigslist founder)

**Workflow:**
1. Analyze user's question
2. Issue targeted search query for specific fact
3. Review search results
4. Repeat until sufficient information gathered
5. Synthesize facts and provide final answer

**Rules:**
- One fact per search query
- Precise, clear search formulation
- Synthesize only after all searches complete
- Language consistency with user's question

---

### 32. Relevant Information Extraction

**Purpose:** Extracts single most relevant piece of information from search results to answer current search query. Focused, concise fact extraction module.

**Location:** `agentic_reasoning/prompts.py` - `RELEVANT_EXTRACTION_PROMPT`

**Usage:** Post-processes search results in reasoning chains, filters noise from retrieved documents.

**Input:**
- Previous reasoning steps (context only)
- Current search query
- Searched web pages

**Output Format:**
- **If answer found**: `Final Information\n[extracted fact]`
- **If no answer**: `Final Information\nNo helpful information found.`

**Rules:**
- Focus exclusively on current search query
- Ignore previous reasoning steps in output
- Be concise, extract only essential facts
- No conversational text, direct factual extraction only

---

## Agent Template Prompts

RAGFlow includes 24 pre-built agent workflow templates in `agent/templates/` (JSON format) with embedded prompts and personas:

**Research & Analysis:**
- **Deep Research** (`deep_research.json`): Multi-agent research with Strategy Research Director (20y experience), Web Search Specialist, Content Reader, Research Synthesizer - produces McKinsey-style reports
- **Deep Search with Reasoning** (`deep_search_r.json`): Deep search with agentic reasoning
- **Knowledge Base Report** (`knowledge_base_report.json`): KB analysis and comprehensive reporting
- **Stock Research Report** (`stock_research_report.json`): Financial/market analysis

**Business Applications:**
- **Customer Service** (`customer_service.json`): Customer support agent
- **Customer Review Analysis** (`customer_review_analysis.json`): Sentiment and feedback analysis
- **CV Analysis** (`cv_analysis_and_candidate_evaluation.json`): Resume screening and candidate evaluation
- **Email Assistant** (`email_assistant.json`): Email composition and management

**Content Creation:**
- **SEO Blog Generator** (`generate_SEO_blog.json`): Search-optimized blog writing
- **Slogan Generator** (`slogan_generator.json`): Marketing slogan creation
- **Technical Support Request** (`technical_support_request.json`): Support ticket handling

**Data & Code:**
- **SQL Assistant** (`sql_assistant.json`): SQL query generation and database interaction
- **Arxiv Research** (`arxiv_research.json`): Academic paper search and analysis
- **Python Coder** (`python_coder.json`): Code generation assistant

**Other Templates:**
- Trip Planner, Paper Proofreading, Reading Companion, Sourcing Agent, Math Tutor, Technical Documentation QA, Multi-Agent Debate, Story Writer, Corrector, and more

Each template defines:
- Agent personas and system prompts
- Tool orchestration (Retrieval, LLM, Categorize, Web Search, SQL, etc.)
- Workflow graph (nodes and edges)
- Parameter configurations

---

## Summary

**Total Documented Prompts: 32 categories + 24 agent templates**

**By Theme:**
- **RAG & Chat**: 11 prompts (citations, questions, keywords, filters, tagging, translation)
- **Agent Workflows**: 8 prompts (task analysis, planning, reflection, memory management)
- **Document Processing**: 6 prompts (vision LLM, TOC detection/extraction)
- **GraphRAG**: 5+ prompts (entity extraction, community reports, mind maps)
- **Agentic Reasoning**: 2 prompts (multi-hop reasoning, information extraction)
- **Agent Templates**: 24 pre-built workflow templates

**Key Technologies:**
- Jinja2 templating for dynamic content
- JSON/Markdown structured outputs
- Multi-language support
- Token budget management
- Streaming and batch processing

**Primary Use Cases:**
- Retrieval-Augmented Generation (RAG)
- Knowledge Graph Construction
- Multi-Agent Workflows
- Document Understanding
- Complex Question Answering
- Content Generation & Analysis

---

*For implementation details, see `rag/prompts/generator.py` (prompt application functions) and `rag/prompts/template.py` (prompt loading utilities).*

